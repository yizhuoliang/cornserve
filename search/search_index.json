{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cornserve","text":"Cornserve: Easy, Fast, and Scalable Multimodal AI <p>Cornserve is an execution platform for multimodal AI. It performs model fission and automatic sharing of common components across applications on your infrastructure.</p> <ul> <li> <p> Model fission</p> <p>Split up your complex models into smaller components and scale them independently.</p> </li> <li> <p> Automatic sharing</p> <p>Common model components are automatically shared across applications.</p> </li> <li> <p> Multimodal-native</p> <p>Cornserve is built multimodal-native from the ground up. Image, video, audio, and text are all first-class citizens.</p> </li> <li> <p> Deploy to K8s with one command</p> <p>One-command deployment to Kubernetes with Kustomize.</p> </li> <li> <p> Observability</p> <p>Built-in support for OpenTelemetry to monitor your apps and requests.</p> </li> <li> <p> Open Source, Apache-2.0</p> <p>Cornserve is open-source with the Apache 2.0 license and is available on GitHub.</p> </li> </ul>"},{"location":"architecture/","title":"Cornserve Architecture","text":""},{"location":"architecture/#cornserve-architecture","title":"Cornserve Architecture","text":"<p>Cornserve is a disaggregated ML serving platform that allows you to implement and deploy ML applications on your infrastructure.</p> <p>Important</p> <p>This document includes descriptions of Cornserve that has not been developed yet. Please track issues on GitHub for the latest updates.</p>"},{"location":"architecture/#task-and-app","title":"Task and App","text":"<p>Applications are written by developers using the Cornserve frontend library in <code>cornserve.frontend</code>. Currently, a Cornserve app is a single Python file that implements three classes and an async function: - <code>Request</code> (inherits from <code>cornserve.app.base.AppRequest</code>): A single input request for the app. - <code>Response</code> (inherits from <code>cornserve.app.base.AppResponse</code>): A single output response for the app. - <code>Config</code> (inherits from <code>cornserve.app.base.AppConfig</code>): Configuration parameters and task definitions for the app. - <code>async def serve(request: Request) -&gt; Response</code>: The main function that handles the request and returns a response.</p> <pre><code>from cornserve.app.base import AppRequest, AppResponse, AppConfig\nfrom cornserve.task.builtins.mllm import MLLMInput, MLLMTask, Modality\n\n\nclass Request(AppRequest):\n    \"\"\"App request model.\n\n    Attributes:\n        prompt: The prompt to send to the LLM.\n        multimodal_data: List of tuples (modality, data URL).\n    \"\"\"\n\n    prompt: str\n    multimodal_data: list[tuple[str, str]] = []\n\n\nclass Response(AppResponse):\n    \"\"\"App response model.\n\n    Attributes:\n        response: The response from the LLM.\n    \"\"\"\n\n    response: str\n\n\nmllm = MLLMTask(\n    model_id=\"Qwen/Qwen2-VL-7B-Instruct\",\n    modalities=[Modality.IMAGE],\n)\n\n\nclass Config(AppConfig):\n    \"\"\"App configuration model.\"\"\"\n\n    tasks = {\"mllm\": mllm}\n\n\nasync def serve(request: Request) -&gt; Response:\n    \"\"\"The function that will be run when a request hits the app.\"\"\"\n    mllm_input = MLLMInput(prompt=request.prompt, multimodal_data=request.multimodal_data)\n    mllm_output = await mllm(mllm_input)\n    return Response(response=mllm_output.response)\n</code></pre> <p>Importantly, in app configurations, apps specify the tasks that they intend to invoke. These tasks are dispatched to be executed on the data plane. There are built-in tasks under <code>cornserve.task.builtins</code>, such as <code>MLLMTask</code> for multimodal LLM inference, <code>LLMTask</code> for LLM inference, and <code>EncoderTask</code> for multimodal data embedding, and users can build their own tasks using components from <code>cornserve.task.base</code>. All other inline Python code is executed in place by the Cornserve Gateway.</p> <p>See also the dedicated page on tasks.</p>"},{"location":"architecture/#control-plane","title":"Control Plane","text":"<p>The control plane manages numerous registered apps and handles incoming requests to each app.</p> <p>Control plane components generally send and receive control signals using gRPC (<code>proto/v1/*.proto</code>). On the other hand, application requests and task invocations are sent and received using HTTP.</p>"},{"location":"architecture/#gateway-and-app-manager","title":"Gateway and App Manager","text":"<p>Package: <code>cornserve.services.gateway</code></p> <p>The gateway is the entry point for all apps and incoming requests to each app.</p> <p>An app is registered with Cornserve by the cluster admin by sending a request to the gateway, including the app's Python source code as string. The gateway then validates the app definition (primarily whether it has all the required classes and the <code>serve</code> function) and registers it with the App Manager singleton class.</p> <p>When a new app is registered, the App Manager will read in the tasks that the app intends to invoke and instruct the Resource Manager to deploy Task Managers in the data plane such that all tasks invoked by the new app is available for execution in the data plane. There is only a single Task Manager per task, so multiple apps that invoke the same task will share a single Task Manager.</p> <p>When a request for a registered app is received, the gateway will spawn a new App Driver for the app to handle the request. The App Driver will execute the app's <code>serve</code> function, and invoked tasks will be sent to the Task Dispatcher, which handles actually executing the task in the data plane and retrieving results back to the App Driver.</p>"},{"location":"architecture/#resource-manager","title":"Resource Manager","text":"<p>Package: <code>cornserve.services.resource_manager</code></p> <p>The Resource Manager is primarily responsible for allocating cluster resources (primarily GPUs) to Task Managers.</p> <p>There are two primary events that trigger the Resource Manager to allocate resources:</p> <ol> <li>New app registration. When a new app is registered and its required tasks are sent to the Resource Manager, the Resource Manager figures out the Task Managers that need to be deployed in the data plane. If there was already an app that required some of the tasks, those Task Managers will not be deployed again, but rather shared by those apps.</li> <li>App unregistration. When an app is unregistered, the Resource Manager will check if there are any other apps that require the same tasks. If not, those unnecessary Task Managers will be killed.</li> </ol> <p>Beyond app registration and unregistration, the Resource Manager also dynamically adjusts the amount of resources given to each Task Manager. Say, if a certain Task Manager receives more requests than others, or if it is computationally heavy and cannot serve as many requests per second compared to other tasks, the Resource Manager will dynamically provision more resources for it. This will happen at the cost of taking away resources from other Task Managers, if need be. The goal would be to balance the request throughput of the whole system over time given a fixed amount of resource.</p>"},{"location":"architecture/#task-manager","title":"Task Manager","text":"<p>Package: <code>cornserve.services.task_manager</code></p> <p>A Task Manager is responsible for executing a single unit task given a subset of the cluster's resources and exposing information about their performance characteristics. A task, for instance, can be an LLM inference task with a particular model; an LLM inference task with a different model, for instance, is considered a different task.</p> <p>Task Managers spawn one or more Task Executors that will actually perform task execution on GPUs on the data plane. The Task Manager is responsible for managing the lifecycle of the Task Executors, including spawning and killing them as needed. When there are more than one Task Executors deployed under a Task Manager, the Task Manager will also route and load balance requests across the Task Executors.</p> <p>For multimodal data embedding tasks, the Task Manager will use Eric as the Task Executor by default. For LLM inference tasks, the Task Manager will use our fork of vLLM as the Task Executor.</p> <p>The Task Manager also exposes performance characteristics of the Task Executors. For instance, given \\(N\\) GPUs, the Task Manager will profile the Task Executor's throughput and latency and expose the throughput--latency Pareto frontier. The Resource Manager can make better resource allocation decisions based on this information.</p>"},{"location":"architecture/#task-dispatcher","title":"Task Dispatcher","text":"<p>Package: <code>cornserve.services.task_dispatcher</code></p> <p>App Drivers or interactive Jupyter notebook users send task invocation requests to the Task Dispatcher, which is responsible for dispatching the requests to appropriate Task Executors and retrieving the results back to the App Driver.</p> <p>When a composite task is invoked, the following happens: 1. The composite task's <code>__call__</code> method records the unit task invocations and dispatches all of them to the Task Dispatcher.     - For instance, if the composite task is a Vision-Language Model task, its <code>__call__</code> method will record two unit task invocations: one for the image encoder and one for the LLM text generation. 2. (For each unit task invocation) The Task Dispatcher queries the Task Manager for the Task Executor that is best suited to handle the request. 3. (For each unit task invocation) The Task Dispatcher translates the <code>TaskInput</code> object of the unit task invocation into JSON, dispatches the HTTP request to the selected Task Executor, waits for the result, and translates the result back to a <code>TaskOutput</code> object. 4. Finally, the Task Dispatcher aggregates all unit task invocation results and response with the final result.</p> <p>How does the Task Dispatcher know how to contact the Task Manager? Whenever there is a change to Task Managers (spawning new ones or killing existing ones), the Resource Manager will inform the Task Dispatcher of the mapping between the unit task definition and its corresponding Task Manager's endpoint.</p>"},{"location":"architecture/#data-plane","title":"Data Plane","text":"<p>The data plane is where the actual task execution happens on GPUs.</p>"},{"location":"architecture/#task-executor","title":"Task Executor","text":"<p>Package: <code>cornserve.task_executors</code></p> <p>As detailed in the Task page, a single unit task class is associated with a Task execution descriptor, which provides information about how to spin up the Task Executor and how to execute the task, among other things.</p> <p>Refer to Eric and vLLM for more information about built-in Task Executors.</p>"},{"location":"architecture/#sidecar","title":"Sidecar","text":"<p>Package: <code>cornserve.services.sidecar</code></p> <p>Data plane Task Executors need to communicate tensor data between each other. A concrete example would be Eric sending the encoded image/video tensor to vLLM for text generation. See Sidecar for more about the Sidecar service.</p>"},{"location":"architecture/eric/","title":"Eric","text":""},{"location":"architecture/eric/#eric-multimodal-data-embedding-server","title":"Eric: Multimodal Data Embedding Server","text":"<p>Mosharaf: Hey, what is this \"Eric\" thing in the architecture diagram? Jae-Won: Oh, uh no, it says \"Enc.\" For Encoder. Mosharaf: Oh. Jae-Won: Now it's Eric.</p> <p>Package: <code>cornserve.task_executors.eric</code></p> <p>Eric is a multimodal data embedding server that takes in a list of multimodal data (e.g., images, videos) and computes the multimodal embedding of the input data.</p>"},{"location":"architecture/eric/#architecture","title":"Architecture","text":"<p>Below, components are divided at the process boundary.</p>"},{"location":"architecture/eric/#router","title":"Router","text":"<p>The gateway router is an async FastAPI server that (1) receives modality encoding requests and (2) preprocesses modality data before running the encoder. Preprocessing is done asynchronously in a thread pool by the <code>eric.router.processor.Processor</code> class.</p> <p>Each model processes different modality data differently, so the router must instantiate the correct model-specific preprocessor. Instantiating and invoking these model- and modality-specific preprocessors are implemented in the class <code>eric.models.[model_module].ModalityProcessor</code>, which is a subclass of <code>eric.models.base.BaseModalityProcessor</code>.</p> <p>When modality preprocessing is complete, the router submits the embedding request to the engine. The router and the engine communicate through ZMQ sockets. Especially, the router holds an instance of the engine client (<code>eric.engine.client.EngineClient</code>), which is used to send requests to the engine and receive responses.</p>"},{"location":"architecture/eric/#engine","title":"Engine","text":"<p>From the engine and below, everything is synchronous Python (i.e., not <code>asyncio</code>).</p> <p>The Engine constantly receives embedding requests from the router, runs the request scheduler to create a <code>eric.schema.Batch</code>, and invokes the model executor (<code>eric.executor.executor.ModelExecutor</code>) to compute the multimodal embedding. The model executor provides the <code>execute_model</code> method, which broadcasts input batch data to all Workers via shared memory.</p> <p>The engine currently only batches data of the same modality together. This is because there are models that have different code paths for different modalities. Furthermore, due to the compute-intensive nature of multimodal encoders, it is unlikely we will scale to large batch sizes.</p>"},{"location":"architecture/eric/#workers","title":"Workers","text":"<p>There is one worker (<code>eric.executor.worker.Worker</code>) process per GPU. The number of workers is the tensor parallelism degree. When spawned, the workers initialize PyTorch distributed and instantiate the model from weights downloaded from the Hugging Face Hub. It then waits for the model executor to dispatch a batch to it, runs tensor parallel inference, and dispatches tensor communication to the destination Task Executor via the sidecar.</p>"},{"location":"architecture/sidecar/","title":"Sidecar","text":""},{"location":"architecture/sidecar/#sidecar-p2p-communication-service","title":"Sidecar: P2P Communication Service","text":"<p>Package: <code>cornserve.services.sidecar</code> (server) and <code>cornserve.sidecar.api</code> (client)</p> <p>Sidecar is a P2P communication service in Cornserve that allows task executors to send/receive intermediate data to/from each other. It's mainly designed for tensors, but it also supports byte-serializable Python objects like strings.</p> <p>Sidecars are intended to be integrated into any Task Executor that needs to communicate with other Task Executors.</p>"},{"location":"architecture/sidecar/#rationale","title":"Rationale","text":"<p>One obvious way to do P2P communication of tensors across the cluster is to use NCCL. However, there are several problems with this approach:</p> <ol> <li>NCCL has a fixed world size that cannot be changed. Faults in one rank will disrupt the whole cluster.</li> <li>NCCL is bound to the process that creates the communicator. This means that NCCL is not amenable to Cornserve auto-scaling Task Manager resources which lead to Task Executors being killed and spawned.</li> <li>NCCL spawns a high-speed polling CUDA kernel that takes up the GPU's SM, potentially leading to performance degradation for actual computation tasks that should be running on the GPU.</li> <li>NCCL may also use NVLink for tensor transfer, which can interfere with the NVLink bandwidth needs for Task Executors (e.g., tensor parallelism).</li> </ol> <p>Instead, each Task Executor runs alongside a long-running Sidecar server, and performs P2P communication with other Task Executors via the sidecar servers. This liberates the Task Executors from the constraints of NCCL.</p>"},{"location":"architecture/sidecar/#architecture","title":"Architecture","text":"<p>Sidecars have servers and clients. Servers are long running services in the cluster, created upon cluster deployment. Clients live within Task Executors, and task executors invoke clients for send and receive operations which are then fulfilled by the servers.</p>"},{"location":"architecture/sidecar/#servers","title":"Servers","text":"<p>Each GPU in the cluster is usually paired with one dedicated Sidecar server, but Sidecar servers can also act as a group when, for example, a task executor runs a model with tensor parallel.</p> <p>All Sidecar servers and clients send control signals through gRPC, while Sidecar servers use <code>UCX</code> as the backend for tensor transfer, which uses RDMA if possible. Small objects are directly sent over through gRPC to reduce contention.</p>"},{"location":"architecture/sidecar/#forwarding-tensors","title":"Forwarding Tensors","text":"<p>Tensor transfer among Sidecar servers does not use NVLink to reserve it for distributed inference like tensor parallelism. Throughout a tensor forward from a producer with a Sidecar sender server to a consumer with a Sidecar receiver server, the Sidecar sender will copy the tensor from the producer's GPU to CPU, and the tensor will arrive at the receiver server's CPU. Therefore, the consumer has the responsibility for copying the received tensor to its devices. If the producer and the consumer locates within the same node, there will be no addition transfer over the network.</p> <p>When multiple Sidecars are grouped, Sidecars assume each producer in the group holds a full replica of the tensor to forward, and the Sidecar Servers could choose to either use one single GPU or use every GPU in the group when copying -- adjusted through a configuration knob.</p>"},{"location":"architecture/sidecar/#chunking","title":"Chunking","text":"<p>Producers are free to chunk the forwarding tensors in any way. However, it's not recommended to have chunks with non-contiguous memory view due overhead. Sidecars view each chunk as independent, so there is no guarantee that all the chunks will be in order or are placed in a slab of contiguous memory. Consumers can decide to process chunks in order, or decide to process all chunks together if the consumer cannot utilize chunks independently.</p>"},{"location":"architecture/sidecar/#memory-management","title":"Memory Management","text":"<p>Sidecar servers manage CPU memory for placing the tensors to send and receive. To reduce internal fragmentation, sidecar clients, thus task executors, are currently required to provide memory hint for the servers. The memory hint is conceptually the memory allocation unit size for the servers, and typically this could be the hidden size of a model the executor is running.</p>"},{"location":"architecture/sidecar/#clients","title":"Clients","text":"<p>The frontend API for Task Executors to interact with servers.</p> <p>Task executors can instantiate a <code>SidecarConfig</code> from <code>cornserve.sidecar.schema</code> and then instantiate a <code>Sidecar</code> client from <code>cornserve.sidecar.api</code>. The client will setup the Sidecar server for the task executor's use upon creation. The client mainly provides three sets of APIs, namely, <code>send</code>, <code>recv</code>, and <code>mark_done</code>.</p>"},{"location":"architecture/sidecar/#send","title":"<code>send</code>","text":"<p><code>send</code> can be used to broadcast some data to a list of Sidecar groups. When chunking is involved, the producer need to fill in the <code>chunk_id</code> and <code>num_chunks</code> parameters.</p>"},{"location":"architecture/sidecar/#recv","title":"<code>recv</code>","text":"<p><code>recv</code> can be used to receive data at chunk-granularity, where <code>chunk_id</code> can be specified. The returning data is either a tensor with CPU storage or a small python object. Receive operations are idempotent for Sidecars, so multiple consumer processes can consume the data concurrently. There is also a synchronous version called <code>recv_sync</code>.</p>"},{"location":"architecture/sidecar/#mark_done","title":"<code>mark_done</code>","text":"<p><code>mark_done</code> is used to free the backing memory of a received tensor in the Sidecar server. As the Sidecar server allows for idempotent receive operations, the data is always held within the server until a corresponding <code>mark_done</code> called.</p>"},{"location":"architecture/task/","title":"Task","text":""},{"location":"architecture/task/#task-abstraction","title":"Task Abstraction","text":"<p>This page explains the definition of tasks as a unit of work and how the system executes them.</p>"},{"location":"architecture/task/#task","title":"<code>Task</code>","text":"<p>A <code>Task</code> is a reusable and composable unit of work that is executed within the data plane. Applications define and invoke one or more Tasks to achieve their goals. A <code>Task</code> logically describes what should be done.</p> <p>A task can be either a Unit Task, or a Composite Task. The former is a single atomic unit of work, while the latter is a composition of multiple Unit Tasks.</p>"},{"location":"architecture/task/#examples","title":"Examples","text":"<p>A <code>Task</code> can be:  </p> <ul> <li>a single inference of a neural network on GPUs (e.g., Text encoder, Vision encoder, Audio encoder, LLM, DiT, VAE decoder, Vocoder)</li> <li>a composition of the above inference units (e.g., a Vision-Language model, a Thinker-Talker architecture model)</li> </ul> <p>Concretely, <code>MLLMTask</code> is a composition of <code>EncoderTask</code> and <code>LLMTask</code>.</p>"},{"location":"architecture/task/#properties","title":"Properties","text":""},{"location":"architecture/task/#recursive-composition","title":"Recursive Composition","text":"<p>Tasks are recursively composed; a Task can be a single inference of a neural network on GPUs or a DAG of other Tasks that make up a larger chunk of coherent work.</p>"},{"location":"architecture/task/#data-forwarding","title":"Data Forwarding","text":"<p>The inputs and outputs to a Task are defined by the Task itself, and both are stored in the App Driver. However, intermediate data (particularly tensors) are forwarded to next Tasks within the data plane via the Sidecar. That is, when a Task is composed of multiple sub-Tasks, the output of one sub-Task is forwarded to the next sub-Task in the DAG.</p>"},{"location":"architecture/task/#static-graph-given-task-input","title":"Static Graph Given Task Input","text":"<p>The concrete execution DAG of a Task must be statically determined at the time of invocation by a request. That is, the DAG must be completely determined by the App Driver given the request to the app. In other words, there can not be any dynamic control flow that depends on unmaterialized intermediate data. For instance, the input to a Task may hold a field <code>image_url: str | None</code>, and whether the execution DAG includes the image encoder can be determined by inspecting whether the <code>image_url</code> field is <code>None</code> or not.</p>"},{"location":"architecture/task/#specification","title":"Specification","text":"<p>The core specification of a Task is by its execution DAG. Each node is a Task instance that has:</p> <ul> <li>Task execution descriptor: Descriptor instance that describes how the Task is executed.</li> <li>The <code>invoke</code> method: The method that executes the Task. Input and output are Pydantic models. The Python code in <code>invoke</code> puts together the Tasks invocation to implicitly define the execution DAG.</li> </ul>"},{"location":"architecture/task/#taskexecutiondescriptor","title":"<code>TaskExecutionDescriptor</code>","text":"<p>A <code>TaskExecutionDescriptor</code> strategy class that describes how a Task is executed. Each concrete <code>Task</code> subclass is associated with one <code>TaskExecutionDescriptor</code> subclasses and takes an instance of the descriptor as an argument to its constructor.</p>"},{"location":"architecture/task/#examples_1","title":"Examples","text":"<p>The <code>LLMTask</code> is compatible with the <code>VLLMDescriptor</code>, which describes how to execute the LLM task using vLLM. Currently, only vLLM is implemented, but other executors like TensorRT-LLM or Dynamo can be implemented in the future. Similarly, the <code>EncoderTask</code> is compatible with the <code>EricDescriptor</code>, which describes how to execute the encoder task using Eric.</p>"},{"location":"architecture/task/#task-lifecycle","title":"Task Lifecycle","text":""},{"location":"architecture/task/#registration","title":"Registration","text":"<p>Unit Tasks classes (e.g., <code>LLMTask</code>) are registered with the whole system. Their source code (concrete class definition) should be available to all services in the system. At the moment, we create multiple built-in Unit Task classes under <code>cornserve.task.builtins</code>.</p>"},{"location":"architecture/task/#deployment","title":"Deployment","text":"<p>A Unit Task class that is registered in the system can be deployed on the data plane as a Unit Task instance (e.g., <code>LLMTask(model_id=\"llama\")</code>).</p> <ol> <li>The Unit Task object is instantiated externally, and then serialized into JSON via Pydantic.</li> <li>The name of the Unit Task (as registered in the system) and the serialized JSON are sent to the Gateway service.</li> <li>The Gateway service sends the unit task instance to the Resource Manager, which ensures that the Task Manager for the Unit Task is running on the data plane.</li> <li>When the Task Manager is running, the Resource Manager notifies the Task Dispatcher with the unit task instance and Task Manager deployment information.</li> </ol> <p>Deployed Unit Tasks become invocable, either as part of Composite Tasks or directly. Invocation can be driven by a static App driver registered in the Gateway service, a human user via our Jupyter Notebook interface.</p>"},{"location":"architecture/task/#invocation","title":"Invocation","text":"<p>Task invocations go to the Task Dispatcher by calling and awaiting on the async <code>__call__</code> method of the Task. This internally calls all <code>invoke</code> methods of Tasks in the DAG, where each unit Task constructs a <code>TaskInvocation</code> object (task, input, and output) to add to a task-specific <code>TaskContext</code> object. The list of <code>TaskInvocation</code> objects are sent to the Taks Dispatcher.</p> <p>The Task Dispatcher is responsible for actually constructing requests, dispatching them to Task Executors, waiting for the results to come back, and then returning task outputs to the App Driver.  </p>"},{"location":"architecture/task/#deregistration","title":"Deregistration","text":"<p>When an App is unregistered and if there are no other active Apps that require the Task, the Resource Manager will kill the Task Manager and free up the resources.</p>"},{"location":"contributor_guide/","title":"Contributor Guide","text":""},{"location":"contributor_guide/#contributor-guide","title":"Contributor Guide","text":"<p>Here, we provide more info for contributors. General principles are here, and child pages discuss specific topics in more detail.</p> <p>We have a few principles for developing Cornserve:</p> <ol> <li>Strict type annotations: We enforce strict type annotation everywhere in the Python codebase, which leads to numerous benefits including better reliability, readability, and editor support. We use <code>pyright</code> for type checking.</li> <li>Automated testing: We don't aim for 100% test coverage, but non-trivial and/or critical features should be tested with <code>pytest</code>.</li> </ol>"},{"location":"contributor_guide/#contributing-process","title":"Contributing process","text":"<p>Important</p> <p>By contributing to Cornserve, you agree that your code will be licensed with Apache 2.0.</p> <p>If the feature is not small or requires broad changes over the codebase, please open an issue at our GitHub repository to discuss with us.</p> <ol> <li>Fork our GitHub repository. Make sure you clone with <code>--recurse-submodules</code> to get the submodules.</li> <li>Create a new Conda environment with something along the lines of <code>conda create -n cornserve python=3.11</code> and activate it with something like <code>conda activate cornserve</code>.</li> <li>Install Cornserve in editable mode with <code>pip install -e 'python[dev]'</code>. If your environment does not have GPUs, you can use <code>pip install -e 'python[dev-no-gpu]'</code>.</li> <li>Generate Python bindings for Protobuf files with <code>bash scripts/generate_pb.sh</code>.</li> <li>Implement changes in your branch and add tests as needed.</li> <li>Ensure <code>bash python/scripts/lint.sh</code> and <code>pytest</code> passes. Note that many of our tests require GPU.</li> <li>Submit a PR to the main repository. Please ensure that CI (GitHub Actions) passes.</li> </ol>"},{"location":"contributor_guide/#developing-on-kubernetes","title":"Developing on Kubernetes","text":"<p>Cornserve runs on top of Kubernetes, which introduces some complexity in development. Please refer to the guide on Local and Distributed Development on Kubernetes for more details.</p>"},{"location":"contributor_guide/#documentation","title":"Documentation","text":"<p>The documentation is written in Markdown and is located in the <code>docs</code> folder. We use MkDocs to build the documentation and use the <code>mkdocs-material</code> theme.</p> <p>To install documentation build dependencies:</p> <pre><code>pip install -r docs/requirements.txt\n</code></pre> <p>To build and preview the documentation:</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"contributor_guide/eric/","title":"Eric","text":""},{"location":"contributor_guide/eric/#eric-developer-guide","title":"Eric developer guide","text":""},{"location":"contributor_guide/eric/#docker-container","title":"Docker container","text":"<p>All code is to be run inside a Docker container, including tests.</p> <pre><code>docker build -t cornserve/eric:latest -f docker/eric.Dockerfile .\ndocker run -it --gpus all --entrypoint bash --ipc host --rm --name eric-dev -v $PWD:/workspace/cornserve -v $HF_CACHE:/root/.cache/huggingface cornserve/eric:latest\n</code></pre>"},{"location":"contributor_guide/eric/#editable-installation","title":"Editable installation","text":"<pre><code>pip install -e 'python[dev]'\n</code></pre>"},{"location":"contributor_guide/eric/#testing","title":"Testing","text":"<p>We use <code>pytest</code>. Tests use GPUs.</p> <pre><code>pytest\n</code></pre> <p>Set the <code>CORNSERVE_TEST_DUMP_TENSOR_DIR</code> to an existing directory when running <code>pytest</code>. This will dump output embedding tensors to the specified directory. Refer to <code>build_batch</code> in <code>tests/task_executors/eric/utils.py</code>.</p> <pre><code>export CORNERSERVE_TEST_DUMP_TENSOR_DIR=/path/to/dump\npytest python/tests/task_executors/eric/models/test_llava_onevision.py::test_image_inference\n</code></pre>"},{"location":"contributor_guide/kubernetes/","title":"Developing on Kubernetes","text":""},{"location":"contributor_guide/kubernetes/#local-and-distributed-development-on-kubernetes","title":"Local and Distributed Development on Kubernetes","text":""},{"location":"contributor_guide/kubernetes/#local-development","title":"Local development","text":"<p>You are developing on a single node. In this case, we don't need a registry. Instead, we build containers directly within the containerd runtime of K3s.</p> <p>First, follow this guide (Section \"Switching from Docker to Containerd\") to set up Nerdctl and BuildKit on your local development machine.</p> <p>After that, you can use Nerdctl to build images directly within K3s containerd, and no pull is necessary whatsoever. Use the <code>build_export_images.sh</code> script with the <code>REGISTRY</code> environment variable set to <code>local</code> (a special case):</p> <pre><code>REGISTRY=local bash scripts/build_export_images.sh \n</code></pre> <p>Use the <code>local</code> overlay to deploy Cornserve:</p> <pre><code>kubectl apply -k kustomize/cornserve-system/overlays/local\nkubectl apply -k kustomize/cornserve/overlays/local\n</code></pre> <p>The <code>local</code> overlay specifies <code>imagePullPolicy: Never</code>, meaning that if the image was not found locally, it means that it was not built yet, correctly raising an error.</p> <p>Note</p> <p>You can use the <code>local</code> overlay for the quick Minikube demo as well.</p>"},{"location":"contributor_guide/kubernetes/#distributed-development","title":"Distributed development","text":"<p>You are developing on a multi-node cluster.</p> <p>(1) Now, you do need a registry to push images to, so that remote nodes can pull them:</p> <pre><code>bash kubernetes/registry.sh\nREGISTRY=myregisty.com:5000 bash kubernetes/set_registry.sh  # (1)!\n</code></pre> <ol> <li>Modifies <code>kustomization.yaml</code> and <code>k3s/registries.yaml</code>    If you're on this dev workflow with a single node cluster, you can skip <code>kubernetes/set_registry.sh</code> because things default to <code>localhost:5000</code>.</li> </ol> <p>(2) For K3s to work with insecure (i.e., HTTP) registries, you need to set up the <code>registries.yaml</code> file in <code>/etc/rancher/k3s/</code> on all nodes (master and worker) before starting K3s:</p> <pre><code>sudo cp kubernetes/k3s/registries.yaml /etc/rancher/k3s/registries.yaml\nsudo systemctl start k3s  # or k3s-agent\n</code></pre> <p>(3) Build and push images to the registry using the <code>build_export_images.sh</code> script with the <code>REGISTRY</code> environment variable set to the registry address:</p> <pre><code>REGISTRY=myregistry.com:5000 bash scripts/build_export_images.sh\n</code></pre> <p>(4) Use the <code>dev</code> overlay (which specifies <code>imagePullPolicy: Always</code>) to deploy Cornserve:</p> <pre><code>kubectl apply -k kustomize/cornserve-system/overlays/dev\nkubectl apply -k kustomize/cornserve/overlays/dev\n</code></pre>"},{"location":"contributor_guide/sidecar/","title":"Sidecar","text":""},{"location":"contributor_guide/sidecar/#sidecar-developer-guide","title":"Sidecar developer guide","text":""},{"location":"contributor_guide/sidecar/#docker-container","title":"Docker container","text":"<p>It is recommended to run everything inside docker. Sidecar uses <code>UCX</code> as backend, so you might find the <code>docker/dev.Dockerfile</code> helpful. Additionally, Sidecar has  dependency over <code>ucxx-cu12</code>, meaning you need to development on an Nvidia GPU-enabled machine at the moment.</p> <p>Specifying <code>--shm-size</code> with at least 4 GB and <code>--ipc host</code> is required.</p>"},{"location":"contributor_guide/sidecar/#editable-installation","title":"Editable installation","text":"<pre><code>pip install -e 'python[dev]'\n</code></pre>"},{"location":"contributor_guide/sidecar/#testing","title":"Testing","text":"<p>We use pytest.</p> <pre><code>pytest python/tests/services/sidecar/test_sidecar.py\n</code></pre>"},{"location":"contributor_guide/tracing/","title":"Tracing","text":""},{"location":"contributor_guide/tracing/#tracing-developer-guide","title":"Tracing Developer guide","text":"<p>We employ OpenTelemetry for observability. Below are some of the conventions we use.</p> <p>Generally, we use auto-instrumentation provided by OpenTelemetry, e.g., FastAPI, gRPC, HTTPX.</p>"},{"location":"contributor_guide/tracing/#spans","title":"Spans","text":"<p>Usually named with <code>ClassName.function_name</code>.</p>"},{"location":"contributor_guide/tracing/#attributes","title":"Attributes","text":"<p>Usually named with <code>namespace.subroutine.attribute_name</code>. <code>namespace</code> is typically the name of the service, like <code>gateway</code>.</p>"},{"location":"contributor_guide/tracing/#events","title":"Events","text":"<p>Usually named with <code>action.event_name</code>. Use spans for things that happen over time (e.g., a subroutine), where tracking the start and end is important. On the other hand, use events for singular occurrences that happen at a specific moment in time.</p>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#getting-started","title":"Getting Started","text":""},{"location":"getting_started/#try-it-out-in-minikube","title":"Try it out in Minikube!","text":"<p>You can try out Cornserve on your local machine (with Docker and at least two NVIDIA V100 GPUs) using Minikube.</p> <p>First, install Minikube following their guide.</p> <p>Then, start a Minikube cluster with GPU support (1):</p> <ol> <li>We recommend enabling rootless docker to avoid permission or <code>$PATH</code> related issues.</li> </ol> <pre><code>minikube start \\\n    --driver docker \\\n    --container-runtime docker \\\n    --gpus all \\\n    --disk-size 50g  # (1)!\n</code></pre> <ol> <li>Give it enough disk space to download model weights and stuff. You can also give more CPU (e.g., <code>--cpus 8</code>) and memory (<code>--memory 16g</code>).</li> </ol> <p>Next, and this is important, we want to increase the shared memory (<code>/dev/shm</code>) size of the Minikube container.</p> <pre><code>$ minikube ssh -- sudo mount -o remount,size=16G /dev/shm\n</code></pre> <p>Next, clone the Cornserve GitHub repository and deploy Cornserve on your Minikube cluster:</p> <pre><code>git clone https://github.com/cornserve-ai/cornserve.git\ncd cornserve\n\nminikube kubectl -- apply -k kubernetes/kustomize/cornserve-system/overlays/minikube\nminikube kubectl -- apply -k kubernetes/kustomize/cornserve/overlays/minikube\n</code></pre> <p>We'll be using Gemma 3 4B for this demo, so you need to have access (requests are processed immediately with an account). While we wait for the containers to spin up, add your HuggingFace access token to Cornserve, which can be created here if you don't have one already. </p><pre><code>minikube kubectl -- create -n cornserve secret generic cornserve-env --from-literal=hf-token='YOUR_HUGGINGFACE_TOKEN'\n</code></pre> <p>After a few moments (which largely depends on how long it takes to pull Docker images from Docker Hub), check whether Cornserve is running:</p> <pre><code>$ minikube kubectl -- get -n cornserve pods   # (1)!\nNAME                               READY   STATUS    RESTARTS   AGE\ngateway-6c65745c5d-x8gkh           1/1     Running   0          4s\nresource-manager-9b4df4687-9djc4   1/1     Running   0          4s\ntask-dispatcher-9954cffcd-g4rk2    1/1     Running   0          4s\nsidecar-0                          1/1     Running   0          3s\nsidecar-1                          1/1     Running   0          3s\nsidecar-2                          1/1     Running   0          3s\nsidecar-3                          1/1     Running   0          3s\n</code></pre> <ol> <li>The number of Sidecar pods should match the number of GPUs you gave to Minikube. They are spawned by the Resource Manager, so you will initially see only three (Gateway, Resource Manager, and Task Dispatcher) pods running.</li> </ol> <p>Next, install the Cornserve CLI that helps you interact with Cornserve:</p> <pre><code># Configure a virtual environment with Python 3.11+ as needed.\npip install cornserve\n</code></pre> <p>Try registering a simple example app that defines a Vision-Language Model:</p> <pre><code>export CORNSERVE_GATEWAY_URL=$(minikube service -n cornserve gateway-node-port --url)\ncornserve register examples/mllm/app.py --alias mllm\n</code></pre> <p>You can check out what the app looks like on GitHub.</p> <p>This will take a few minutes. The two large bits are (1) pulling in the Docker images and (2) waiting for vLLM to warm up and start. But eventually, you should see something like this:</p> <pre><code>$ cornserve register examples/mllm/app.py --alias mllm\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 App ID                               \u2502 Alias \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 app-564b79ff446342c69821464b22585a72 \u2502 mllm  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Now, you can invoke the app using the CLI:</p> <pre><code>$ cornserve invoke mllm - &lt;&lt;EOF\nprompt: \"Write a haiku about each image.\"\nmultimodal_data:\n- [\"image\", \"https://picsum.photos/id/12/480/560\"]\n- [\"image\", \"https://picsum.photos/id/234/960/960\"]\nEOF\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 response \u2502 Okay, here are haikus for each image:                                     \u2502\n\u2502          \u2502                                                                           \u2502\n\u2502          \u2502 **Image 1: Coastal Landscape**                                            \u2502\n\u2502          \u2502                                                                           \u2502\n\u2502          \u2502 Gray sea meets the shore,                                                 \u2502\n\u2502          \u2502 Rocks stand still, a weathered grace,                                     \u2502\n\u2502          \u2502 Island dreams unfold.                                                     \u2502\n\u2502          \u2502                                                                           \u2502\n\u2502          \u2502 **Image 2: Paris Scene**                                                  \u2502\n\u2502          \u2502                                                                           \u2502\n\u2502          \u2502 Fog veils city\u2019s height,                                                  \u2502\n\u2502          \u2502 Eiffel stands, a ghostly trace,                                           \u2502\n\u2502          \u2502 Winter\u2019s quiet grace.                                                     \u2502\n\u2502          \u2502                                                                           \u2502\n\u2502          \u2502 ---                                                                       \u2502\n\u2502          \u2502                                                                           \u2502\n\u2502          \u2502 Would you like me to create haikus for any other images you have in mind? \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The invocation payload and response schema are defined by the app itself as a <code>AppRequest</code> and <code>AppResponse</code> subclass. You can learn more about defining apps (and tasks) in our guide.</p> <p>Here's how to clean up:</p> <pre><code>minikube kubectl -- delete -k kubernetes/kustomize/cornserve/overlays/minikube\nminikube kubectl -- delete -k kubernetes/kustomize/cornserve-system/overlays/minikube\nminikube stop  # or minikube delete\n</code></pre>"},{"location":"getting_started/#getting-started-seriously","title":"Getting started (seriously)","text":"<p>At a high level, there are two steps to using Cornserve:</p> <ol> <li>Cornserve deployment: Deploying Cornserve on a GPU cluster managed by Kubernetes.</li> <li>Building your app: Building a Cornserve app and deploying it on a Cornserve cluster for invocation.</li> <li>Interactively debugging your app with Jupyter notebook: Building a Cornserve app and deploying it on a Cornserve cluster for invocation.</li> <li>Registering and invoking your app: Building a Cornserve app and deploying it on a Cornserve cluster for invocation.</li> </ol>"},{"location":"getting_started/building_apps/","title":"Building Apps","text":""},{"location":"getting_started/building_apps/#building-and-deploying-your-app","title":"Building and Deploying Your App","text":"<p>Cornserve as two layers of defining execution:</p> <ul> <li>App: This is the highest level of construct, which takes a request and returns a response. Apps are written in Python and can be submitted to the Cornserve Gateway for deployment.</li> <li>Task: This is a unit of work that is executed by the Cornserve data plane. There are two types of tasks:<ul> <li>Unit Task: Unit Tasks are the smallest and most basic type of task. They are executed in a single Kubernetes Pod and are the unit of scaling. For instance, there is the built-in modality embedding unit task which embeds specific modalities (e.g., image, video, audio), which is executed by our Eric server. There is also the built-in LLM text generation task, which generates text from input text prompts and any embedded modalities.</li> <li>Composite Task: Composite Tasks are a composition of one or more Unit Tasks. They are defined by the user in Python. For instance, there is the built-in Multimodal LLM composite task which instantiates modality embedding unit tasks as needed, runs then on multimodal data to embeds them, and passes them to the LLM text generation unit task to generate text. Intermediate data produced by unit tasks are forwarded directly to the next unit task in the graph.</li> </ul> </li> </ul>"},{"location":"getting_started/building_apps/#example-writing-an-image-understanding-app","title":"Example: Writing an Image Understanding App","text":"<p>Apps are written in Python and use Tasks to process requests. Let's build a simple example app that takes an image and a text prompt, and generates a response based on the image and the prompt.</p>"},{"location":"getting_started/building_apps/#composite-task","title":"Composite Task","text":"<p>First, let's see how to build a composite task out of built-in unit tasks for this: <code>ImageChatTask</code>.</p> <pre><code>from cornserve.task.base import Task, TaskInput, TaskOutput\nfrom cornserve.task.builtins.encoder import EncoderTask, Modality, EncoderInput\nfrom cornserve.task.builtins.llm import LLMTask, LLMInput\nfrom cornserve.app.base import AppRequest, AppResponse, AppConfig\n\n\nclass ImageChatInput(TaskInput):\n    prompt: str\n    image_url: str\n\n\nclass ImageChatOutput(TaskOutput):\n    response: str\n\n\nclass ImageChatTask(Task[ImageChatInput, ImageChatOutput]):\n    model_id: str\n\n    def post_init(self) -&gt; None:\n        \"\"\"Initialize subtasks.\"\"\"\n        self.image_encoder = EncoderTask(\n            model_id=self.model_id,\n            modality=Modality.IMAGE,\n        )\n        self.llm = LLMTask(model_id=self.model_id)\n\n    def invoke(self, task_input: ImageChatInput) -&gt; ImageChatOutput:\n        \"\"\"Invoke the task.\"\"\"\n        encoder_input = EncoderInput(data_urls=[task_input.image_url])\n        image_embedding = self.image_encoder.invoke(encoder_input)\n        llm_input = LLMInput(\n            prompt=task_input.prompt,\n            multimodal_data=[(\"image\", task_input.image_url)],\n            embeddings=[embedding for embedding in image_embeddings.embeddings],\n        )\n        llm_output = self.llm.invoke(llm_input)\n        return ImageChatOutput(response=llm_output.response)\n</code></pre> <p>It was a handful of code, so let's break it down:</p> <ol> <li>Input/Output Models: We define <code>ImageChatInput</code> and <code>ImageChatOutput</code> using Pydantic. This allows us to define clear input and output models for our task. These should inherit from <code>TaskInput</code> and <code>TaskOutput</code>, respectively.</li> <li>Task Class: We define a new composite task class called <code>ImageChatTask</code> that inherits from <code>Task[ImageChatInput, ImageChatOutput]</code>. This class specifies two things:<ul> <li>Subtasks, namely the built-in <code>EncoderTask</code> and <code>LLMTask</code>, which are instantiated in the <code>post_init()</code> method. This is where we define the subtasks that will be used in the task.</li> <li>Task logic. Each unit task (e.g., <code>EncoderTask</code>) expects its input data to be an instance of its <code>TaskInput</code> (e.g., <code>EncoderInput</code>), and returns an instance of its <code>TaskOutput</code> (e.g., <code>EncoderOutput</code>). The <code>invoke</code> method is where we define the logic of how the subtasks are composed together to produce the final output.</li> </ul> </li> </ol>"},{"location":"getting_started/building_apps/#app","title":"App","text":"<p>With <code>ImageChatTask</code> defined, we can now use it in our app:</p> <pre><code>from cornserve.app.base import AppRequest, AppResponse, AppConfig\n\nimage_chat = ImageChatTask(model_id=\"Qwen/Qwen2-VL-7B-Instruct\")\n\n\nclass Request(AppRequest):\n    image_url: str\n    prompt: str\n\n\nclass Response(AppResponse):\n    response: str\n\n\nclass Config(AppConfig):\n    tasks: {\"image_chat\": image_chat}\n\n\nasync def serve(request: Request) -&gt; Response:\n    \"\"\"App's main entry point that serves a request.\"\"\"\n    image_chat_input = ImageChatInput(\n        prompt=request.prompt,\n        image_url=request.image_url,\n    )\n    image_chat_output = await image_chat(image_chat_input)\n    return Response(response=image_chat_output.response)\n</code></pre> <p>This app only uses a single composite task, <code>ImageChatTask</code>, but it should be easy to see that you can use arbitrary numbers of unit and composite tasks in your app.</p> <p>Another thing to note is that the app's main entry point is an async function called <code>serve</code>. This is the function that will be called by the Cornserve Gateway when a request is received.</p> <p>Finally, notice that when you compose tasks inside composite tasks, you called the <code>invoke</code> method of tasks synchronously. However, in the context of apps, you call the <code>__call__</code> method of tasks asynchronously. This allows you to run multiple tasks in parallel with usual Python asynchronous programming patterns like <code>asyncio.gather</code>.</p>"},{"location":"getting_started/building_apps/#debugging","title":"Debugging","text":"<p>We've just showed how to build a simple app. However, having the build the entire thing in one shot is not the most convenient. In the next page, we'll show how you can interactively build and debug your task and app logic in Jupyter Notebook!</p>"},{"location":"getting_started/cornserve/","title":"Deploying Cornserve","text":""},{"location":"getting_started/cornserve/#deploying-cornserve","title":"Deploying Cornserve","text":"<p>Cornserve can be deployed on a GPU cluster managed by Kubernetes.</p> <p>Note</p> <p>The <code>cornserve</code> namespace is used for most of our control plane and data plane objects. On the other hand, the <code>cornserve-system</code> namespace is used for components that look over and manage the Cornserve system itself (under <code>cornserve</code>), like Jaeger and Prometheus.</p> <p>If you already have a Kubernetes cluster running, you can deploy Cornserve on it with the <code>prod</code> overlay:</p>"},{"location":"getting_started/cornserve/#deploying-k3s","title":"Deploying K3s","text":"<p>Tip</p> <p>If you have a Kubernetes cluster running, you can skip this section.</p> <p>If you don't have a Kubernetes cluster running, you can deploy Cornserve on a K3s cluster. We also use the K3s distribution of Kubernetes for our development. Refer to their Documentation for more details.</p> <p>Tip</p> <p>If you're deploying on-premise with k3s, make sure you have plenty of disk space under <code>/var/lib/rancher</code> because <code>containerd</code> stores images there. If not, you can create a directory in a secondary storage (e.g., <code>/mnt/data/rancher</code>) and symlink it to <code>/var/lib/rancher</code> prior to starting k3s.</p>"},{"location":"getting_started/cornserve/#nvidia-device-plugin","title":"NVIDIA Device Plugin","text":"<p>The NVIDIA GPU Device Plugin is required to expose GPUs to the Kubernetes cluster as resources. You can deploy a specific version like this:</p> <pre><code>kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.17.2/deployments/static/nvidia-device-plugin.yml\n</code></pre>"},{"location":"getting_started/cornserve/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone git@github.com:cornserve-ai/cornserve.git\ncd cornserve/kubernetes\n</code></pre>"},{"location":"getting_started/cornserve/#master-node","title":"Master Node","text":"<p>Install and start K3s:</p> <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_ENABLE=true sh -\nsudo mkdir -p /etc/rancher/k3s\nsudo cp k3s/server-config.yaml /etc/rancher/k3s/config.yaml\nsudo systemctl start k3s\n</code></pre> <p>Note the master node address (<code>$MASTER_ADDRESS</code>) and the node token (<code>$NODE_TOKEN</code>):</p> <pre><code>NODE_TOKEN=\"$(sudo cat /var/lib/rancher/k3s/server/node-token)\"\n</code></pre>"},{"location":"getting_started/cornserve/#worker-nodes","title":"Worker Nodes","text":"<p>Install and start K3s:</p> <pre><code>curl -sfL https://get.k3s.io | K3S_URL=https://$MASTER_ADDRESS:6443 K3S_TOKEN=$NODE_TOKEN INSTALL_K3S_SKIP_ENABLE=true sh -\nsudo mkdir -p /etc/rancher/k3s\nsudo cp k3s/agent-config.yaml /etc/rancher/k3s/config.yaml\nsudo systemctl start k3s-agent\n</code></pre>"},{"location":"getting_started/cornserve/#deploying-cornserve_1","title":"Deploying Cornserve","text":"<p>If you haven't already, clone the Cornserve repository:</p> <pre><code>git clone git@github.com:cornserve-ai/cornserve.git\ncd cornserve\n</code></pre> <p>On top of a Kubernetes cluster, you can deploy Cornserve with a single command:</p> <pre><code>kubectl apply -k kubernetes/kustomize/cornserve-system/base\nkubectl apply -k kubernetes/kustomize/cornserve/overlays/prod\n</code></pre> <p>Note</p> <p>The <code>cornserve</code> namespace is used for most of our control plane and data plane objects. On the other hand, the <code>cornserve-system</code> namespace is used for components that look over and manage the Cornserve system itself (under <code>cornserve</code>), like Jaeger and Prometheus.</p>"},{"location":"getting_started/jupyter/","title":"Using Jupyter Notebook","text":"<p>In the previous page, we showed an example of building a simple image understanding app. However, one-shotting the entire app in Python isn't very convenient. Instead, for non-trivial apps, you can interactively build the app logic in a Jupyter notebook. Here's an example that uses the built-in composite task <code>MLLMTask</code>.</p> In\u00a0[1]: Copied! <pre>from cornserve.task.builtins.mllm import MLLMTask, Modality\nmllm = MLLMTask(\n    model_id=\"Qwen/Qwen2-VL-7B-Instruct\",\n    modalities=[Modality.IMAGE],\n)\n</pre> from cornserve.task.builtins.mllm import MLLMTask, Modality mllm = MLLMTask(     model_id=\"Qwen/Qwen2-VL-7B-Instruct\",     modalities=[Modality.IMAGE], ) <p>You can deploy this task to the cluster so that you can later invoke it. To deploy, you need to create a <code>CornserveClient</code>.</p> In\u00a0[2]: Copied! <pre>from cornserve.frontend import CornserveClient\ncornserve = CornserveClient(url=\"your-endpoint:port\")\n</pre> from cornserve.frontend import CornserveClient cornserve = CornserveClient(url=\"your-endpoint:port\") <pre>Connected to Cornserve gateway at your-endpoint:port\n</pre> <p>Deploy may take a few minutes at first.</p> In\u00a0[3]: Copied! <pre>cornserve.deploy(mllm)\n</pre> cornserve.deploy(mllm) Out[3]: <pre>TaskResponse(status=200, content='Tasks declared used')</pre> <p>The builtin <code>MLLMTask</code> takes <code>MLLMInput</code>, so you can construct the input and try to invoke it. Note the <code>multimodal_data</code> field is a list of tuples.</p> In\u00a0[4]: Copied! <pre>from cornserve.task.builtins.mllm import MLLMInput\nmllm_input = MLLMInput(\n    prompt=\"What is this image about?\",\n    multimodal_data=[(\"image\", \"https://picsum.photos/seed/random/200/300\")]\n)\n</pre> from cornserve.task.builtins.mllm import MLLMInput mllm_input = MLLMInput(     prompt=\"What is this image about?\",     multimodal_data=[(\"image\", \"https://picsum.photos/seed/random/200/300\")] ) In\u00a0[5]: Copied! <pre>await mllm(mllm_input)\n</pre> await mllm(mllm_input) <pre>INFO 2025-04-30 14:58:12,737 [cornserve.task.base:509] Dispatching tasks to http://your-endpoint:port/tasks/invoke\n</pre> Out[5]: <pre>MLLMOutput(response='The image shows a dog wrapped in a blanket. The blanket appears to be plaid or checkered, with a mix of colors including beige, pink, and green. The dog is sitting on the ground, and the blanket is wrapped around its body, covering its head and part of its back. The background suggests an outdoor setting, possibly a forest or a park, with trees and greenery visible. The overall scene gives a cozy and warm impression, as if the dog is being kept warm in the cold weather.')</pre> <p>Now we can build more complex Tasks. For example:</p> In\u00a0[6]: Copied! <pre>from cornserve.task.base import Task, TaskInput, TaskOutput\nclass MyTaskInput(TaskInput):\n    \"\"\"Task input model.\"\"\"\n\n    first_prompt: str\n    second_prompt: str\n    multimodal_data: list[tuple[str, str]] = []\n\nclass MyTaskOutput(TaskOutput):\n    \"\"\"Task output model.\"\"\"\n    response: str\n\nclass MyTask(Task):\n    def __init__(self):\n        super().__init__()\n        self.mllm = MLLMTask(\n            model_id=\"Qwen/Qwen2-VL-7B-Instruct\",\n            modalities=[Modality.IMAGE],\n        )\n\n    def invoke(self, input: MyTaskInput) -&gt; MyTaskOutput:\n        \"\"\"Invoke the task with the given input.\"\"\"\n        mllm_input = MLLMInput(\n            prompt=input.first_prompt,\n            multimodal_data=input.multimodal_data,\n        )\n        mllm_output = self.mllm.invoke(mllm_input)\n\n\n        input = MLLMInput(\n            prompt=input.second_prompt,\n            multimodal_data=input.multimodal_data,\n        )\n\n        output = self.mllm.invoke(input)\n        return MyTaskOutput(response=f\"{mllm_output.response} \\n-----\\n{output.response}\")\n</pre> from cornserve.task.base import Task, TaskInput, TaskOutput class MyTaskInput(TaskInput):     \"\"\"Task input model.\"\"\"      first_prompt: str     second_prompt: str     multimodal_data: list[tuple[str, str]] = []  class MyTaskOutput(TaskOutput):     \"\"\"Task output model.\"\"\"     response: str  class MyTask(Task):     def __init__(self):         super().__init__()         self.mllm = MLLMTask(             model_id=\"Qwen/Qwen2-VL-7B-Instruct\",             modalities=[Modality.IMAGE],         )      def invoke(self, input: MyTaskInput) -&gt; MyTaskOutput:         \"\"\"Invoke the task with the given input.\"\"\"         mllm_input = MLLMInput(             prompt=input.first_prompt,             multimodal_data=input.multimodal_data,         )         mllm_output = self.mllm.invoke(mllm_input)           input = MLLMInput(             prompt=input.second_prompt,             multimodal_data=input.multimodal_data,         )          output = self.mllm.invoke(input)         return MyTaskOutput(response=f\"{mllm_output.response} \\n-----\\n{output.response}\") <p>We can deploy it and test it. Note the deployment finishes instantly because <code>Cornserve</code> can reuse the <code>UnitTask</code>s in each <code>Task</code>.</p> In\u00a0[7]: Copied! <pre>my_task = MyTask()\ncornserve.deploy(my_task)\n</pre> my_task = MyTask() cornserve.deploy(my_task) Out[7]: <pre>TaskResponse(status=200, content='Tasks declared used')</pre> In\u00a0[8]: Copied! <pre>my_input = MyTaskInput(\n    first_prompt=\"What's the name of the flower?\",\n    second_prompt=\"Write a haiku about the image\",\n    multimodal_data=[(\"image\", \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/Crab_apple_flower_4004.jpg/2880px-Crab_apple_flower_4004.jpg\")],\n)\nresult = await my_task(my_input)\nprint(result.response)\n</pre> my_input = MyTaskInput(     first_prompt=\"What's the name of the flower?\",     second_prompt=\"Write a haiku about the image\",     multimodal_data=[(\"image\", \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/Crab_apple_flower_4004.jpg/2880px-Crab_apple_flower_4004.jpg\")], ) result = await my_task(my_input) print(result.response) <pre>INFO 2025-04-30 14:58:17,938 [cornserve.task.base:509] Dispatching tasks to http://your-endpoint:port/tasks/invoke\n</pre> <pre>The flower in the picture is a Crab Apple (Malus). Crab apples are a type of apple tree known for their small, often colorful flowers that bloom in early spring. They are not typically grown for their fruit, which is often too small and sour for eating, but are valued for their ornamental value and the attractive flowers they produce. \n-----\nSoft pink petals,\nGentle leaves whispering,\nSpring's gentle breath.\n</pre> <p>We no longer need the <code>mllm</code>, so can <code>teardown</code> it in the cluster. Note this returns instantly because its <code>UnitTask</code> are also used by <code>my_task</code>!</p> In\u00a0[9]: Copied! <pre>cornserve.teardown(mllm)\n</pre> cornserve.teardown(mllm) Out[9]: <pre>TaskResponse(status=200, content='Tasks declared not used')</pre> <p>The <code>CornserveClient</code> uses a web socket to talk to the Cornserve Gateway. A disconnect will automatically teardown all the tasks you deployed.</p> In\u00a0[10]: Copied! <pre>cornserve.close()\n</pre> cornserve.close() <pre>Closed connection to Cornserve gateway.\nClosed keep-alive thread.\n</pre>"},{"location":"getting_started/jupyter/#using-jupyter-notebook","title":"Using Jupyter Notebook\u00b6","text":""},{"location":"getting_started/registering_apps/","title":"Registering and Invoking Apps","text":""},{"location":"getting_started/registering_apps/#deploying-apps-to-cornserve-and-invoking-them","title":"Deploying Apps to Cornserve and Invoking Them","text":"<p>Once you've written your app, you can deploy it to Cornserve. The current deployment process is as follows:</p> <ol> <li>Save the app code in a single Python file (e.g., <code>image_chat.py</code>).</li> <li>Register &amp; deploy the app to the Cornserve Gateway for validation and deployment:     <pre><code>export CORNSERVE_GATEWAY_URL=[...]\ncornserve register image_chat.py\n</code></pre></li> <li>When validation succeeds, the Cornserve Gateway will deploy the app and all its subtasks on the Cornserve data plane, and the <code>cornserve</code> CLI invocation will return with the app's ID.</li> <li>Finally, you can send requests to the Cornserve Gateway with your choice of HTTP client.     <pre><code>response = requests.post(\n    f\"{CORNSERVE_GATEWAY_URL}/app/invoke/{APP_ID}\",\n    json={\n        \"request_data\": {\n            \"image_url\": \"https://example.com/image.jpg\",\n            \"prompt\": \"Describe the image.\",\n        }\n    },\n)\n</code></pre>     Notice that what comes within the <code>\"request_data\"</code> key is the JSON representation of your <code>Request</code> class defined in our previous example.</li> </ol>"},{"location":"getting_started/registering_apps/#next-steps","title":"Next Steps","text":"<p>To dive deeper into the architecture of Cornserve, check out our architecture guide.</p>"}]}